# 6  确保公平并解决偏见的根源
机器学习模型通常会表现出令人惊讶和意想不到的行为。 当此类行为导致对用户产生不良影响的模式时，我们可能会根据一些标准将模型归类为“不公平”。例如，如果具有相似特征的人得到的结果完全不同，则这违反了个人公平的标准[149]。如果某些敏感群体（种族，性别等）得到不同的结果模式—例如不同的假阴性率，则可能违反人口统计学公平性的各种标准，参见[48，300]。反事实公平的标准要求在考虑所有因果相关途径之后，如果用户是不同组的成员（种族，性别等），则应获得与他们相同的待遇[250]。  
        联邦学习为公平性研究带来了一些机会，其中一些扩展了非联邦环境下的先前研究方向，而另一些则是联邦学习所独有的。本节在这两个类别中都提出了未解决的问题。



## 6.1  训练数据偏差
机器学习模型中不公平的一个驱动因素是训练数据中的偏差，包括认知，抽样，报告和确认偏差。一种常见的反模式是训练数据中少数民族或边缘化社会群体的代表不足，因此学习者在训练期间对这些群体的加权较小[222]，导致对这些群体成员的预测质量较差（例如[85]）。  
		正如联邦学习中使用的数据访问过程可能会引入数据集移位和非独立性一样（第3.1节），也存在引入偏差的风险。例如：

+ 如果在插入电源或充满电后选择设备进行更新，则在一天中不同时间计算出的模型更新和评估结果可能会与诸如日班与夜班工作时间表之类的因素相关联。
+ 如果在给定时间从合格设备池中选择了要更新的设备，则在连接了很少其他设备（例如夜班或异常时区）时连接的设备可能会在聚合输出中过度表示。
+ 如果当输出计算更快时选定的设备更有可能保留其输出，则：a）来自处理器速度更快的设备的输出可能被过度代表，这些设备可能是较新的设备，因此与社会经济地位相关； b）数据量较少的设备可能会被过度代表，这些设备可能代表较少使用该产品的用户。
+ 如果数据节点拥有不同数量的数据，那么联邦学习可能会对大量使用产品或生成数据特性的人群给予更高的贡献权重
+ 如果出于社会经济原因，潜在用户群体不拥有设备，那么在训练数据集中他们可能代表性不足，随后在模型训练和评估中的代表性也可能不足（或没有）。
+ 在联邦训练期间，跨选定设备的模型损失的未加权汇总可能不利于某些设备上的模型性能[263]。  

​        已经观察到，数据生成过程中的偏差也可能导致从该数据中学到的结果模型的不公平性（例如，参见[150，338]）。例如，假设训练数据基于用户与产品的交互，而该产品未能纳入包容性设计原则。然后，与产品的用户交互可能不会表达用户意图（因此应针对其进行优化），而可能会围绕非包容性产品设计表达应对策略（因此可能需要对产品交互模型进行基本修复）。从这样的交互中学习可能会忽略或永久保留某些产品用户群体的不良体验，而这种方式很难在联邦环境中维护隐私的同时进行检测。在所有机器学习方案中，训练数据都是从用户交互派生的，这种风险是共同承担的，但在联邦环境中，当从单个设备上的应用程序收集数据时，尤其要注意这一风险。  
​        对于联邦学习研究和机器学习研究而言，调查可识别或减轻数据生成过程中偏差的程度是一个关键问题。同样，尽管有限的先前研究已经证明了在联邦环境中识别和纠正已经收集的数据中的偏差的方法（例如，通过[268]中的对抗方法），但仍需要在这一领域进行进一步的研究。 最后，将事后公平校正应用于从可能有偏差的训练数据中学习模型的方法，也是未来工作的重要方向。



## 6.2  不获取敏感属性下的公平性  
明确访问人口统计信息（种族，性别等）对于许多现有的公平性标准（包括第6.1节中讨论的公平性标准）至关重要。 但是，当个人敏感属性无法得到时，经常使用联邦学习的环境也会引起对公平性的考虑。例如，在开发个性化语言模型或开发公平的医学图像分类器而又不知道有关个人的任何其他人口统计学信息时，可能会发生这种情况。在没有有关敏感群体成员资格的数据的情况下，衡量和纠正不公平现象都是联邦学习研究人员要解决的关键领域。  
        有限的现有研究已经检查了不获取敏感属性下的公平性。例如，这已通过使用分布式稳健优化（DRO）解决，该优化针对训练期间所有个体的最坏情况结果进行了优化[199]，并通过多重校准对训练数据子集之间的公平性进行了校准[202]。甚至这些现有方法都没有在联邦环境中应用，这为将来的经验工作提供了机会。如何使这些方法适用于联邦环境下典型的大规模，高维数据的挑战也是一个悬而未决的问题，因为DRO和多重校准都带来了使用大n和p进行缩放的挑战。最后，在不考虑“敏感属性”的情况下，开发其他理论方法来定义公平性是进一步研究的关键领域。  
        解决此问题的其他方法包括重新定义现有的公平概念，这些概念主要与均衡结果的概率有关（对于受影响的个人，其中一种被认为是“积极”，另一种被认为是“消极”）。取而代之的是，无法访问敏感属性的公平性可能被重新定义为对有效模型的平等访问。在这种对公平的解释下，目标是在所有个体中最大化模型效用，而不论他们（未知）的人口身份，也不考虑单个结果的“优缺点”。同样，这与最常使用联邦学习的环境（例如语言建模或医学图像分类）相匹配，在这种情况下，对于用户来讲什么是好的结果没有明确的概念，相反，目标只是为用户提供正确预测，不管结果如何。  
        现有的联邦学习研究提出了满足这种公平性解释的可能方法，例如通过个性化[217，403]。 在[263]中采用了类似的公平概念，即“模型性能在设备之间的更公平分布”。
        明确地使用属性无关的方法来确保公平的模型性能对于未来的联邦学习研究是一个开放的机会，尤其重要的是，随着联邦学习达到成熟，看到更多的采用真实的用户群进行部署，过程中不需要了解用户的敏感身份。 

 

## 6.3  公平性，隐私性和鲁棒性  
公平和数据隐私似乎是互补的道德概念：在许多需要保护隐私的现实世界中，公平也是需要的。 通常这是由于基础数据的敏感性所致。 由于联邦学习最有可能部署在同时需要私密性和公平性的敏感数据环境中，因此重要的是，FL的研究调查了FL如何能够解决机器学习中存在的关于公平的问题，以及FL是否会引起新的公平相关的问题。  
        但是，在某些方面，公平的理想似乎与FL试图为其提供保证的隐私概念相抵触：差分隐私学习通常试图掩盖个人识别特征，而公平通常要求了解个人在敏感领域的身份以此来衡量或确保做出公正的预测。尽管已经在非联邦环境中研究了隐私差异与公平之间的权衡[214,127]，但关于FL如何（或是否）能够唯一解决公平问题的研究很少。  
        最近的证据表明，差分隐私学习会对敏感的群体产生不同的影响[41、127、214、246]，这提供了进一步的动机来调查FL是否能够解决此类问题。一个潜在的缓解隐私（旨在防止模型过于依赖个人）和公平（鼓励模型在代表性不足的类上表现出色）之间的紧张关系的解决方案可能是应用个性化技术(在3.3节已讨论)和“混合差分隐私”，其中一些捐赠数据的用户具有更低的隐私保护[39]。  
        此外，当前的差分隐私优化方案在不考虑敏感属性的情况下应用-从这个角度来看，经验研究表明，差分隐私优化对少数群体的影响最大[41]。对差分隐私优化算法的修改，这些算法明确寻求保留少数群体的性能，例如通过调整噪声和剪切机制以说明数据中群体的表示形式，也很可能会极大地限制差分隐私模型对经过差分隐私训练的联邦模型中少数群体的潜在不同影响。 然而，以提供某种形式的隐私保证的方式来实现这种自适应的差分隐私机制，会带来算法上和理论上的挑战，这需要在未来的工作中加以解决。  
        还需要进一步研究以确定在联邦环境中上述问题的严重程度。此外，如第6.2节所述，当敏感属性不可用时，评估隐私差异对模型公平性的影响尤其困难，因为目前尚不清楚如何识别模型表现不佳的群体并量化隐私差异的“价格” –调查和应对这些挑战是未来工作的一个开放问题。  
        更广泛地说，人们可以更普遍地检查隐私，公平性和鲁棒性之间的关系（请参阅第5节）。之前许多关于机器学习的工作，包括联邦学习，通常都集中在鲁棒性（针对中毒或逃避），隐私或公平性的孤立方面。一个重要的公开挑战是发展对鲁棒，私有和公平的联邦学习系统的共同理解。这种综合方法可以提供机会，从不同但互补的机制中受益。差分隐私机制既可以用来减轻数据推断攻击，又可以为抵抗数据中毒的鲁棒性提供基础。另一方面，这种集成方法也揭示了新的漏洞。例如，最近的工作揭示了隐私和鲁棒性之间的权衡取舍[365]。  
        最后，在学习独立于某些敏感属性的数据表示形式的环境中，隐私和公平自然地相遇，同时保留了所关注任务的效用。确实，可以在隐私方面（要转换数据以隐藏私有属性）和公平性来推动这一目标，这是使在这种表示形式上训练的模型相对于属性较公平的一种方式。 在集中式环境中，学习此类表示的一种方法是通过对抗训练技术，该技术已应用于图像和语音数据[268、164、282、60、367]。 在联邦学习场景中，客户可以将本地的转换应用于其数据，以强制执行或改善FL过程的隐私和/或公平性保证。但是，以联邦方式（可能在隐私和/或公平约束下）学习此转换本身是一个未解决的问题。  



## 6.4  利用联邦来提高模型多样性  
联邦学习为通过分布式训练集成以前在单个位置合并可能不切实际甚至不合法的数据集提供了机会。例如，在美国，《健康保险携带和责任法案》（HIPAA）和《家庭教育权利与隐私法案》（FERPA）分别限制了医疗患者数据和学生教育数据的共享。迄今为止，这些限制已导致在机构孤岛中进行建模：例如，使用来自各个医疗机构的电子健康记录或临床图像，而不是跨机构汇总数据和模型[83，93]。在机构数据集的成员资格与个人特定的敏感属性相关的情况下，或者在更广泛的情况下他们的行为和结果，可能导致用户在这些机构中代表性不足的群体中代表性较差。重要的是，训练数据中缺乏代表性和多样性已被证明会导致较差的表现，例如在遗传疾病模型[286]和图像分类模型[85]中。  
        联邦学习通过提供有效的分散式训练协议以及所生成模型的隐私和不可识别性保证，提供了利用独特多样化数据集的机会。 这意味着，联邦学习可以在以前不可能实现的许多领域中，对多行业数据集进行训练。 这提供了一个实践机会，可以利用更大，更多样化的数据集并探索以前仅限于小规模人群的模型的泛化性。 更重要的是，它提供了一个机会，可以通过组合可能已经与敏感属性相关联的边界数据来提高这些模型的公平性。 例如，参加特定保健或教育机构可能与个人的种族或社会经济地位相关。 如上文第6.1节所述，训练数据中的代表性不足已证明是模型不公平的驱动因素。  
        未来的联邦学习研究应调查在联邦训练环境中改善多样性还可以改善所得模型公平性的程度，以及在此类环境中所需的差分隐私机制可能会限制多样性增加带来的公平性和性能提升的程度。 这包括对需要进行联邦学习的经验研究以及量化多样性，公平性，隐私性和绩效之间相互作用的需求； 理论研究为机器学习公平背景下的多样性等概念奠定了基础。  



## 6.5  联邦公平：新机遇与挑战
值得注意的是，联邦学习为公平研究者提供了独特的机会和挑战。例如，通过允许通过观察(甚至通过特性)分布的数据集，联邦学习可以使用划分后的数据进行建模和研究，而划分后的数据可能过于敏感而不能直接共享[190，198]。以联邦方式使用的数据集的可用性的增加可以帮助改善可用于机器学习模型的训练数据的多样性，从而可以促进公平建模理论和实践。  
        研究人员和从业人员还需要应对联邦学习所带来的与公平相关的独特挑战。例如，联邦学习可以通过基于连接类型/质量，设备类型，位置，活动模式和本地数据集大小等考虑因素决定要采样哪些客户端，从而引入新的偏差来源[74]。未来的工作可能会调查这些各种采样约束对结果模型的公平性的影响程度，以及如何在联邦框架内减轻此类影响，例如[263]。诸如不可知联邦学习[303]之类的框架提供了一种控制训练目标偏差的方法。改进现有联邦训练算法的公平性的工作将特别重要，因为进步已经开始接近FL系统其他组件的技术极限，例如模型压缩，改善现有联邦训练算法的公平性的工作将尤其重要，这最初有助于扩大联邦训练过程中候选客户的多样性。  
        在经典的集中式机器学习环境中，过去十年来在训练分类器方面取得了长足的进步，例如约束优化，移位后方法和分布稳健优化[197，430，199]。 一个公开的问题是，这些已经在集中式训练中有效提高公平性的方法，是否可以在以去中心化方式放置数据的联邦学习的环境下（如果是，在哪些其他假设下）使用这些方法，并且从业人员可能无法从数据中获得与总体分布相匹配的无偏样本。

